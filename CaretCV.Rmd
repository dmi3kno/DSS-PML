---
title: "Understanding caret's cross-validation"
author: "Dmytro Perepolkin"
date: "29. desember 2015"
output: html_document
---

There's a lot of confusion about using different types of cross-validation in data analysis. I will follow the typical process and then try to suggest alternatives. 

## Introduction

This post is inspired by an excellent post by Max Kuhn in his [blog]( http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm). Here's a quote:

> There are quite a few methods for resampling. Here is a short summary (more in Chapter 4 of the book):
>
>* k-fold cross-validation randomly divides the data into k blocks of roughly equal size. Each of the blocks is left out in turn and the other k-1 >blocks are used to train the model. The held out block is predicted and these predictions are summarized into some type of performance measure (e.g. >accuracy, root mean squared error (RMSE), etc.). The k estimates of performance are averaged to get the overall resampled estimate. k is 10 or >sometimes 5. Why? I have no idea. When k is equal to the sample size, this procedure is known as Leave-One-Out CV. I generally don't use it and >won't consider it here.
>
>* Repeated k-fold CV does the same as above but more than once. For example, five repeats of 10-fold CV would give 50 total resamples that are >averaged. Note this is not the same as 50-fold CV.
>
>* Leave Group Out cross-validation (LGOCV), aka Monte Carlo CV, randomly leaves out some set percentage of the data B times. It is similar to >min-training and hold-out splits but only uses the training set.
>
>* The bootstrap takes a random sample with replacement from the training set B times. Since the sampling is with replacement, there is a very strong >likelihood that some training set samples will be represented more than once. As a consequence of this, some training set data points will not be >contained in the bootstrap sample. The model is trained on the bootstrap sample and those data points not in that sample are predicted as hold-outs.

I am interested in k-fold (kF) and Leave-Group-Out (LGO) and will discuss them below.

## Data Preparation

I will re-use recent example from Coursera's final project in the Data Science Specialization's Practical Machine Learning course.

```{r data preparation, echo=FALSE}
library(caret)

rm(list=ls())

if (!file.exists("pml-training.csv")) { 
  file.url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv" 
  download.file(file.url,destfile='pml-training.csv') 
} 

if (!file.exists("pml-testing.csv")) { 
  file.url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv" 
  download.file(file.url,destfile='pml-testing.csv') 
} 

training <- read.csv("pml-training.csv", stringsAsFactors = F, na.strings = c("NA", "", " "))
testing <- read.csv("pml-testing.csv", stringsAsFactors = F, na.strings = c("NA", "", " "))

# removing columns with NAs and columns 1:7
col2rem <- sapply(testing, function(x) {sum(is.na(x)) > 0})
col2rem[1:7] <- TRUE # removing "X", user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window"
training <- training[, !col2rem]
testing <- testing[, !col2rem]

training$classe <- as.factor(training$classe)
lab <- ncol(training) # label is the last column
```

Typically people start with splitting their data into training and holdout set. This is the first thing all aspiring data analysts are taught to do.

```{r splitting the data, message=FALSE }
set.seed(111)
inTrain <- createDataPartition(training$classe, p = 0.7, list = FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
```

But then in courses like **Practial Machine Learning** we are also taught about `caret` package and the beauty of `trainControl()` for automatic cross-validation.

```{r setting up trainControl, message=FALSE }
# defining cross-validation 
set.seed(111)
fitControl <- trainControl(method = "cv", number = 5)
# disabling grid search
rpart.grid <- expand.grid(.cp=0.03743261) 
```

At this time most people just proceed to fit the model.

```{r fitting the model}
# fitting the model
set.seed(111)
fit <- train(x=myTraining[, -lab], y= myTraining[, lab], method = "rpart", trControl = fitControl, tuneGrid=rpart.grid)
fit 
```

Watch! There are some accuracies! Same information is also accessible in `fit$results`. 
What is repored here is simple average of the accuracy per fold.

```{r fold accuracies}
fit$resample
c(sapply(fit$resample[,1:2], mean),sapply(fit$resample[,1:2], sd))

```

On the other hand, there's this confusion matrix embedded in the "fit" object. Where does it come from?

```{r fit confusion table}
cM.fit <- confusionMatrix.train(fit, norm = "none")
cM.fit
```

Each fold has its own respective confusion matrix. The confusionMatrix above is nothing else, but just an "average confusion Matrix"

```{r fold confusion tables}
# Each fold has its own respective confusion matrix
fit$resampledCM

# the above cM.train confusionMatrix is nothing else, but just an "average confusion Matrix"
matrix(apply(fit$resampledCM[,1:25], 2, mean), nrow = 5, byrow = F)
```

We can recreate confusion tables for each fold. Here's for example confusion Matrix for Fold1

```{r fold 1 confusion table}
cM.Fold1 <-  unlist(subset(fit$resampledCM,Resample=="Fold1")[,1:25])
dim(cM.Fold1) <- c(5,5)
cM.Fold1
```

And from this we can calculate the accuracy per fold. Compare it to what was reported in `fit$resample`

```{r fold accuracies calculation}
#and from this we can calculate the accuracy per fold
cM.Fold.Accuracies <- apply(fit$resampledCM[, 1:25], 1, function(x){dim(x) <- c(5,5); sum(diag(x))/sum(x)})
names(cM.Fold.Accuracies) <- fit$resampledCM[, 27]
cM.Fold.Accuracies

# Compare it to what was reported in fit$resample
fit$resample

```

Back to confusionMatrix for Fold1. Relevant question: *"What are these numbers?"*

```{r fold 1 confusion table investigation}
cM.Fold1
```

For that you need to go back to the fit object

```{r fold 1 sizes}
cat("Fold1 was trained on",length(fit$control$index$Fold1), "observations \n")
cat("And tested on", nrow(myTraining)-length(fit$control$index$Fold1), "observations \n")
cat("This is exactly the sum of the Fold1 confusion matrix:", sum(cM.Fold1), "\n")
```

Note, all of the above is out-of-sample accuracies and confusiontables. Of course one could calculate "in-sample" confusionMatrix and respective "in-sample" Accuracy

```{r In-sample Accuracy}
pred.cv <-predict(fit)
confusionMatrix(pred.cv, myTraining$classe)
```

And then, of course, there's the usual *"Leave Group Out"* confusion table and Accuracy. This is what is typically reported as expected out-of-sample accuracy/error.

```{r LGO Accuracy}
pred.holdout <- predict(fit, newdata=myTesting)
confusionMatrix(pred.holdout, myTesting$classe)
```

## Alternative approach 1: no holdout - just cross-validation

Why doing LGo if you can cross-validate? Let's re-fit the model on the whole training set

```{r re-fitting the model on whole training}
dim(training)
set.seed(111)
fit.nho <- train(x=training[, -lab], y= training[, lab], method = "rpart", trControl = fitControl, tuneGrid=rpart.grid)
fit.nho 
```

Note that accuracies are different (lower?) as folds are now bigger. Here's also accuracy per fold.

```{r NHO accuracy per fold}
fit.nho$resample
```

I argue that mean accuracy from this table is what one should expect to get on testing set

```{r average out-of-sample accuracy}  
cat("Average out-of-sample accuracy: ", sapply(fit.nho$resample[,1:2], mean), "\n")
```

## Alternative approach 2: Quick Leave-Group-Out (LGO)

[Max](http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm) calls this approach *"Leave Group Out cross-validation (LGOCV)"*, but I think the name is confusing. There's nothing *"cross-"* about it. Just random split and fitting the model once. Even if the process is repeated, there's no guarantee the splits will change places (holdout will be used for training and vice versa).

But it is, sure, faster! If that is the objective, then save time on cross-validation, just do LGO and move on! We will need new `fitControl` object.

```{r pure LGO} 
fitControl.LGO <- trainControl(method = "none")

# and re-fit the model. It should run much faster now
set.seed(111)
fit.LGO <- train(x=myTraining[, -lab], y= myTraining[, lab], method = "rpart", trControl = fitControl.LGO, tuneGrid=rpart.grid)
fit.LGO
```
There's no cross-validation going on, but since we are not looking at cross-validation, why spend time on it? Just go ahead and predict the holdout set as usual.
```{r pure LGO results} 
fit.LGO$results

pred.holdout.LGO <- predict(fit.LGO, newdata=myTesting)
confusionMatrix(pred.holdout.LGO, myTesting$classe)
```

## Conclusion

In summary, I think, the analyst has two choices:
* if precision is the objective - do k-fold cross-validation on as much data as possible - i.e. the whole training set
* if speed is important - disable cross-validation in `trainControl` and proceed to splitting the training data set into *training* and *holdout* using `createDataPartition`

Perhaps, the latter is preferred for exploratory analysis, while the former is useful once the analyst proceeds to model fine-tuning.